\chapter{Orientaciones generales}

Este capítulo inicia indagando un poco sobre las perspectivas que dieron origen a la mecánica estadística, y finaliza de manera sucinta con la propuesta dada por E.T. Jaynes. A  partir de un recorrido histórico, con el fin de ubicar el nacimiento de la mecánica estadística en un contexto natural, se exploran los puntos de vista de Gibbs y Boltzmann de manera explícita, señalando las dificultades que trae consigo cada uno. Posteriormente, se exponen las implicaciones que tiene la hipótesis de ergodicidad, y se concluye con el postulado entregado por E.T. Jaynes.

\section{Recuento Histórico}

Explicar los puntos de vista más relevantes de la mecánica estadística implica situar un contexto histórico, dado que es la forma más clara y precisa de comprender cómo surgieron estas ideas. Por tal razón, en esta sección se expondrán las circunstancias en las que se formuló la termodinámica; según el libro de I. Muller  \cite{MullerHistory}.
\\
La física actual fue iniciada por Hamilton y Lagrange creando métodos eficaces para enfrentar diferentes problemas. Aunque la mecánica clásica tiene un amplio alcance dando predicciones acordes con los experimentos, se demostró también que esta teoría no daba respuesta a todos los aspectos de la naturaleza. Ya desde el siglo XVIII, empezaba a existir la necesidad de una teoría que tratara conceptos diferentes a los comunes a la mecánica, como el calor y la energía, aunque, inicialmente, estos fueron ideas muy vagas y primitivas. La teoría sobre el calor fue evolucionando gracias a los experimentos de Graf von Rumford, Julius Mayer y James Prescott Joule. Por otro lado, Nicolás Carnot trabajaba en motores de calor y sus consecuencias teóricas, lo que dio el soporte para que un nuevo concepto empezara a circular, \textit{la entropía}. Poco a poco el sentido de la termodinámica fue evolucionando hasta llegar al actual, gracias a autores como Benoît Clapeyron, Lord Kelvin y Rudolf Clausius.
\\
La termodinámica se basó en experimentos y fue organizada bajo postulados \cite{CallenThermo}, sus predicciones fueron respaldadas experimentalmente gracias a la innovación y la industria, lo que le generó una esencia diferente a la de las otras teorías físicas ya que no propone una explicación sobre las leyes, simplemente expone qué ocurre con las propiedades de un sistema macroscópico en equilibrio. Por ejemplo, la primera ley de la termodinámica solo expone cómo la energía del sistema es constante sin importar el proceso que se lleve a cabo. Dado su origen, la termodinámica no se interesaba por encontrar una conexión entre sus resultados y los constituyentes de la materia.
\\
De este problema nace la mecánica estadística. sus esfuerzos iniciales hechos por los científicos de la época se centraron en considerar la materia hecha de partículas y de esta manera, poder predecir todas las propiedades térmicas macroscópicas ya encontradas por la termodinámica. Los autores que más ayudaron a la creación de esta teoría fueron Gibbs, Boltzmann y Maxwell; quienes trazaron las ideas principales y moldearon la teoría para poder tener métodos generales para aplicaciones de diferentes tipos. Fueron tan asertivos los postulados de los métodos producidos por Gibbs, que son los que todavía se utilizan.
\\
\\
\subsection{Termodinámica}
La mecánica estadística es la base conceptual de la termodinámica, aunque históricamente primero apareció la teoría de la termodinámica. La termodinámica apareció gracias a la agrupación de varias ideas en el siglo XIX que encerraban diversos tipos de experimentos y nociones existentes desde la antigua Grecia. A pesar de que las ideas que dieron paso a la termodinámica surgieron en la antigüedad, poco a poco se han ido depurando. Por ejemplo, según I. Muller, Klaudios Galenos (133-200 A.C) suponía que la influencia del clima sobre los fluidos del cuerpo podía determinar el carácter de una persona. Así, decía que los habitantes del norte frío y húmedo eran salvajes y violentos, al contrario de los habitantes del sur, caliente y seco, que eran flácidos y mansos; lo que evidencia de manera clara que la idea de temperatura ha sido conocida intuitivamente de una u otra manera. 
\\
En el año 1578 Johannis Hasler presentó una tabla de las temperaturas corporales de las personas en relación a la altura en la que vivían; actualmente se sabe que la temperatura corporal no depende de la altura sino de otras variables como una buena salud, por ejemplo. Este error conceptual se dio debido a los precarios instrumentos de medición usados en esa época \footnote{los instrumentos de medición de la época se veían afectados con el cambio de presión. Por lo tanto al hacer mediciones de temperatura en diferentes alturas los resultados obtenidos variaban de acuerdo a la misma}. Ya para inicios del siglo XVII se tenía un instrumento que posibilitaba determinar con precisión la temperatura: se trataba del termómetro. Gracias a este instrumento, muchas ideas equivocadas dadas por la subjetividad de los sentidos sobre el calor y el frío se fueron eliminando. En los años 1700 empezaron a aparecer las escalas de temperatura, y hubo incluso 18 escalas en algún momento \cite{MullerHistory}. En 1848 William Thompson (Lord Kelvin) introdujo la escala absoluta denominada la escala Kelvin, la cual cuenta desde el cero absoluto en adelante. El punto de ebullición del agua a 1 atm es 373.15 Kelvin.
\\
Otro, concepto importante de la termodinámica es la energía. En sus inicios se hablaba de calor o fuerza; no se comprendía en si qué era el calor. La teoría calórica pretendía dar explicación al calor. Entre diversas propuestas se encuentran las de Pierre Gassendi (1592-1655) donde proponía que el calor y el frío eran tipos de materia diferente. Según I. Muller, Gassendi consideraba a los átomos del frío como tetraedros que al entrar a un líquido se solidificarían.
\\
Antoine Laurent Lavoisier (1743-1794) consideró al calor como otro elemento: lo contemplaba como un fluido que llamaba el calórico. Una de las primeras personas en cuestionar la teoría calórica fue Graf Von Rumford (1753-1814), observando en unos cañones el cambio del calor liberado dependiendo de si estaban afilados o no. Concluyó, entonces, que el calor debía ser el mismo que había sido administrado debido al movimiento.
\\
Rumford continuó trabajando en su idea del equivalente mecánico del calor: moviendo un cabrestante con dos caballos, notó que el calor del barril era igual $"$al de 9 velas grande$"$. Rumford siguió haciendo experimentos sobre el calor, midió de manera meticulosa el peso del agua antes y después de ser congelada; encontró que el peso no cambiaba, pero aun así se cedía calor en el proceso. Entonces concluyó que si el calórico \footnote{El calórico era la idea anterior al calor, se le consideraba un fluido.} existía era imponderable. Pese a los experimentos realizados por Rumford, la teoría calórica siguió siendo de gran aceptación durante muchos años.  	
\\
Robert Julius Mayer (1814-1878) estudió medicina, pero su entusiasmo hacia la física lo guió a hacer experimentos en este campo. La idea principal que tenía Mayer era que la energía se conservaba de forma general, no solo cuando hay trabajo mecánico, es decir, cualquier fenómeno capaz de aportar energía debía tenerse en cuenta al momento de un experimento. Gracias a los estudios hechos por su cuenta sabía que la energía cinética, en sus palabras $"$fuerza viva$"$, podía convertirse en calor. Experimentalmente dedujo que la caída de un peso a una altura de 365 metros correspondía a calentar el mismo peso de agua de $0^{\circ}$ a $1^{\circ}$. No es muy exacto pero estuvo bastante cerca, incluso llegó a cambiar la altura a 425 metros luego de que Joule hiciera mejores mediciones.
\\
Mayer tenía ideas bastante originales, aunque en general no sabía expresarlas por falta de experiencia matemática y su aislamiento de la comunidad científica. Pero Mayer fue el primero en hablar sobre máquinas térmicas y decir que el calor absorbido por el vapor era siempre mayor que el calor liberado durante la condensación, y su diferencia era trabajo útil. Esta idea se expuso antes que Carnot y Clapeyron, quienes sabían cómo se manejaba el calor pero no conocían su naturaleza.
\\
James Prescott Joule (1818-1889) proporcionó en extremo detalle observaciones sobre el calor y la temperatura. Sus experimentos eran discutidos en sus escritos de forma muy extensa, presentando factibles errores, compensando pérdidas; eran tan detallados que algunos de sus artículos se volvieron estándares. Es por eso que Mayer comparó sus experimentos con los de Joule, encontrando que uno de los aportes de Joule fue que gracias a sus experimentos se aceptó la conservación de la energía.
\\
Posteriormente Hermann Ludwig Ferdinand Helmholtz (1821-1894) propuso la idea que lo que se llamaba calor era la energía cinética del movimiento térmico de los átomos. Siendo Helmhotlz el primero en descartar el término de fuerza viva, y llamarlo energía. El trabajo de Helmholtz fue importante ya que Joule y Mayer no podía dar una visión clara sobre lo que se llamaba calor, y divagaban entre calor y fuerza; Helmholtz planteó con mayor claridad los conceptos en comparación con los otros teóricos.
\\
Es así como, paso a paso, a la luz de estas reflexiones, desaciertos y aciertos, se va construyendo de manera significativa el camino para dar origen a la primera ley de la termodinámica. Como se puede observar, la historia de la energía no tuvo un desarrollo tan claro y asertivo, por lo que llegar hasta la idea actual, no fue para nada rápido ni sencillo. Puede ponerse en contraste con ésta la mecánica cuántica, que aunque fue inspirada también por la radiación del cuerpo negro, un tema que en ese entonces era tratado por la termodinámica, su construcción matemática y teórica avanzó más rápidamente. 
\\
A diferencia de la energía, la entropía fue un concepto más provocador al momento de darle un sentido microscópico. 
\\
La historia de la entropía empieza por los motores térmicos; hay especulaciones de que en el primer siglo antes de Cristo ya existían algunas máquinas que funcionaban con vapor de agua: se cree que Herón de Alexandría hizo una de éstas. En el siglo XVIII Thomas Newcome y Thomas Savery crearon una máquina de vapor que inicialmente ayudaba a sacar agua de las minas, pero fue James Watt(1736-1819) quien mejoró esa máquina de vapor haciéndola de 3 a 4 veces más eficiente.
\\
Nicolas Léonard Sadi Carnot (1796-1832) se preguntó sobre qué tanto se podía optimizar la eficiencia de una máquina térmica. Carnot publicó un libro donde argumenta proposiciones para resolver la pregunta hecha; allí encontró que la eficiencia de una máquina que trabaja entre dos temperaturas y solo intercambia calor entre ellas depende únicamente de la temperatura, $e=F(T)dT$ para un delta de temperatura, incluso aunque las máquinas se manejen con agentes diferentes para generar trabajo.
\\
Carnot seguía creyendo en la teoría calórica. Para llegar a estas ideas no fue necesario saber que esta teoría estaba incorrecta, sus resultados se mantuvieron como ciertos. Según I. Muller, Carnot no pudo localizar exactamente los valores de la eficiencia. Clapeyron y Kelvin, tampoco, pudieron hallar los valores de esta eficiencia ni por mediciones ni por cálculos.
\\
En 1850 se afirmaba que la teoría calórica estaba mal formulada y que se debía hacer algo al respecto. Clausius retocó algunas ideas de Carnot y pudo encontrar la eficiencia del ciclo de Carnot. Clausius le dio forma a la termodinámica actual -incluso los cursos actuales de termodinámica siguen un artículo de Clausius que habla de gases ideales y vapor húmedo \cite{MullerHistory}-. Clausius amplió sus investigaciones a ciclos no infinitesimales y a procesos no reversibles, él fue quien dio la idea de la entropía y sus propiedades. Con eso propuso la segunda ley de la termodinámica: El calor no puede pasar solo de un cuerpo frío a un cuerpo caliente. Clausius llamó $S=\frac{Q}{T}$ a la entropía y vio que esto era lo que se conservaba en un ciclo de Carnot. Pero en términos microscópicos no se conocía el significado de $S$.
\\
\subsection{Teoría Cinética}
Paralelamente a mitad del siglo XIX, se trabajaba en la teoría cinética de los gases. Esta teoría estudiaba propiedades macroscópicas tales como la presión, temperatura, viscosidad, conductividad térmica etc., de un número muy grande de partículas que seguían las ecuaciones clásicas del movimiento. Había dos suposiciones que se planteaban en esta teoría. La primera, que los gases eran sistemas mecánicos de muchas partículas idénticas. La segunda, que debido a la gran cantidad de partículas se debía introducir probabilidades para observar, de esta manera, algunas regularidades que aparecían debido a las configuraciones de las moléculas. Teóricos como Clausius, Maxwell (1831-1879) y Boltzmann (1844-1906) trabajaron en esta área y produjeron resultados importantes. Sin	 embargo, y a pesar de estar de acuerdo con los resultados experimentales, hubo muchas discusiones sobre las hipótesis usadas para llegar a estos resultados.
\\
Así, por ejemplo, en los trabajos de Clausius se hace uso de los siguientes supuestos para gases en reposo y en equilibrio térmico: Las moléculas que están dentro de un recipiente se encuentran distribuidas con la misma densidad en todo el recipiente, la distribución de velocidades es igual en todo el recipiente, todas las direcciones de velocidad son igual de probables. Estas tres hipótesis se constituyeron en el comienzo para ver los sistemas mecánicos desde una perspectiva probabilística \cite{Ehrenfest}.
\\
Maxwell convirtió las ideas de Clausius sobre la poca dispersión de la distribución de velocidad en algo que se podía calcular. Así que Maxwell propuso en 1859 su ley de distribución
\begin{equation}
f(u,v,w) \Delta u \Delta v \Delta w = A e^{-B(u^{2}+v^{2}+w^{2})}  \Delta u \Delta v \Delta w,
\end{equation}
donde $f(u,v,w) \Delta u \Delta v \Delta w $ es el número de moléculas entre esos límites de velocidades, cada límite representa un componente de la velocidad.
\\
Botlzmann en 1868 generalizó el resultado de Maxwell. Tenía la misma situación de un gas en equilibrio y en reposo, pero ahora con un campo de fuerza externo actuando sobre las moléculas. Boltzmann tuvo en cuenta que las moléculas eran compuestas de otras partículas, lo cual afectaba los diferentes valores de energía potencial  \cite{Ehrenfest}, denotando a $\Delta \tau$ como el rango de variaciones muy pequeñas que puede tener el estado de la molécula, o sea $\Delta \tau= \Delta x \Delta y \Delta z \Delta u \Delta v \Delta w$. La generalización dada por Boltzmann es
\begin{equation}
f(x,y,z,u,v,w) \Delta \tau = \alpha e^{-\beta E} \Delta \tau.
\end{equation}
$E$ es la energía total que tiene la molécula; esto es, la energía cinética, energía potencial interna y energía potencial externa. Esta generalización es llamada la distribución de Maxwell-Boltzmann. Boltzmann además mostró que cualquier otra distribución bajo la condición de colisiones entre partículas evoluciona hacia la distribución de Maxwell-Boltzmann. Usó el teorema H para llegar a esta conclusión.
\section{Fundamentos de la Mecánica Estadística}
Es desde este contexto histórico referencial que Boltzmann dio las primeras ideas sobre la mecánica estadística.
\\
En esta sección, se expondrán los conceptos más importantes que Boltzmann postuló y ayudaron a formar la física estadística. La meta de Boltzmann era lograr dar un enfoque microscópico a la termodinámica y poder explicar las propiedades térmicas que se observaban en los sistemas macroscópicos. Como primera conexión intentó darle un significado microscópico a la segunda ley de la termodinámica. Esta conexión viene demostrada por el teorema H.
\\
Aquí es necesario destacar que, para hablar sobre teorema H, primero se debe explicar la ecuación de transporte de Boltzmann. Esta ecuación puede derivarse desde la jerarquía de Bogoliubov-Born-Green-Kirkwood-Yvon o al observar el problema del número de partículas en un rango del estado de una molécula en el espacio de fase \cite{HuangStat}. Para ambas propuestas se debe hacer supuestos para poder seguir con el análisis, lo que no se mostrará en este trabajo ya que no es pertinente seguir todos los pasos. 
\\
La ecuación de transporte de Boltzmann describe cómo la distribución de moléculas $f(\vec{r},\vec{p},t)$ evoluciona en el tiempo. Donde $\vec{r}$ es la posición y $\vec{p}$ es el momento. La ecuación es       	
\begin{align*}
\quad (\frac{\partial}{\partial t}- \frac{\partial U}{\partial \vec{q}_{1}} \cdot \frac{\partial}{\partial \vec{p}_{1}} +\frac{\vec{p}_{1}}{m} \cdot \frac{\partial}{\partial \vec{q}_{1}})f  &=
\\
- \int d^{3}\vec{p}_{2}d^{2} \Omega |\frac{d \sigma}{d \Omega}| |\vec{v}_{1}-\vec{v}_{2}|[f(\vec{p}_{1},\vec{q}_{1},t)f(\vec{p}_{2},\vec{q}_{1},t) &- f(\vec{p}_{1}',\vec{q}_{1},t)f(\vec{p}_{2}',\vec{q}_{1},t)].
\end{align*}
Esta ecuación se puede leer como \cite{KardarStat}: el término a la izquierda describe el movimiento de una partícula en un potencial U, el término de la derecha es la probabilidad de encontrar que una partícula con momento $\vec{p}_{1}$ en $\vec{q}_{1}$ sea alterada por una colisión con una partícula con momento $\vec{p}_{2}$.  La probabilidad de esta colisión está dada por el producto de los factores cinéticos proporcionado por la sección transversal diferencial $ |\frac{d \sigma}{d \Omega}|$, el flujo de partículas incidentes $|\vec{v}_{1}-\vec{v}_{2}|$ y la probabilidad de encontrar dos partículas $f(\vec{p}_{1},\vec{q}_{1},t)f(\vec{p}_{2},\vec{q}_{1},t)$. El primer término substrae la probabilidad e integra sobre todos lo momentos posibles y el ángulo sólido. El segundo término es la adición del proceso inverso. 
\\
Hablar de la ecuación de transporte de Boltzmann es importante para seguir con el teorema H; ya que este dice que: si $f$ satisface la ecuación de transporte de Boltzmann, entonces $\frac{dH}{dt} \leq 0$, donde 
\begin{equation}
H(t)= \int d^{3} \vec{p} d^{3} \vec{q}  f(\vec{p},\vec{q},t) \ln (f(\vec{p},\vec{q},t)).
\end{equation}
Con este teorema se puede demostrar que la distribución en el equilibrio es la distribución de Maxwell-Boltzmann, o sea que cuando $t \to \infty$ la distribución  $f(\vec{p},t) \to f_{0}(\vec{p})=  \alpha e^{-\beta E}$ \cite{HuangStat}. Pero hay un punto importante aquí, y es algo que también en su época desconcertó a los contemporáneos de Boltzmann, ¿cómo es posible llegar a una descripción de fenómenos irreversibles que muestran una dirección temporal partiendo de una teoría de un gas reversible? Esto está dado por las suposiciones hechas al derivar la ecuación de transporte de Boltzmann \cite{Ehrenfest}.
\\
Se usa la hipótesis de caos molecular (Stosszahlanzatz). Esta dice que, después de una colisión, las partículas que interactuaron durante el choque quedan descorrelacionadas. Entonces, por el caos molecular $f(\vec{r},\vec{p}_{1},\vec{p}_{2},t)=f(\vec{r},\vec{p}_{1},t)f(\vec{r},\vec{p}_{2},t)$. Es por esta hipótesis que la simetría temporal se rompe y hace que haya una dirección particular para el tiempo, porque al descorrelacionar las partículas después de un choque se rompe la simetría que tienen las ecuaciones de movimiento clásicas.
El teorema H fue usado por Boltzmann para poder darle una base microscópica a la segunda ley de la termodinámica. La relación es 
\begin{equation}
S(t)=- \frac{H(t)}{k_{B}},
\end{equation}
donde $S$ es la entropía y $k_{B}$ es la constante de Boltzmann. Pero también Boltzmann recibió muchas críticas sobre  la posibilidad de asociar a H con la entropía.
\\
Una de las objeciones que recibió fue sobre la posibilidad de poner todas las velocidades de las moléculas de forma opuesta a las dadas inicialmente, lo que daría resultados contrarios a los esperados. Si inicialmente se encontraba que H decrecía, ahora, al invertir las velocidades, se hallaba que H aumentaba. Esto significa que hay un caso para el cual la entropía disminuye lo cual va en contra de la segunda ley \cite{KhinchinStat}. 
\\
Otro problema dado por E. Zermelo es que Poincaré había demostrado que en ese modelo cinético de un gas aislado, el sistema se comportaba cuasiperiódicamente. Esto quiere decir que la función H puede ir asumiendo valores más grandes después de un periodo de tiempo \cite{Ehrenfest}. 
\\
\section{Gibbs}
Más adelante W. Gibbs (1839-1903) con su libro \textit{Elementary Principles of Statistica Mechanics} en 1901 propuso la forma actual de hacer física estadística. Gibbs desarrolla las ideas de ensambles, el estado micro canónico y canónico. Las ideas de Gibbs eran más imaginativas que las de Boltzmann, porque introdujo conceptos no muy intuitivos a la mecánica estadística. Aunque Gibbs volvió a formular la física estadística con ideas nuevas, también se encontraron algunas dificultades en ellas. \\
Para mostrar las nociones dadas por Gibbs se supone un sistema compuesto de $N$ partículas. Gracias a la mecánica clásica se conocen las ecuaciones de movimiento para un sistema de varias partículas, las ecuaciones de Hamilton:
\begin{equation} \label{Hamilton}
\frac{dq_{i}}{dt}= \frac{\partial H}{\partial p_{i}} \quad , \quad \frac{d p_{i}}{dt} =-\frac{\partial H}{\partial q_{i}},
\end{equation}
donde el índice $i=1,...,3N$ y $H$ es el hamiltoniano del sistema. Se podría pensar que el problema ya se encuentra solucionado pero hay dificultades al tener estas ecuaciones: la cantidad de ecuaciones de movimiento es muy grande, pues un volumen típico es del orden de $10^{23}$ partículas. En la actualidad, aunque las herramientas que proporciona la computación ayudan a resolver muchas ecuaciones suponiendo cierta simplicidad en ellas, aún así existe el problema de las condiciones iniciales del sistema. En la práctica es imposible de encontrar la posición y el momento de todas las partículas para el mismo tiempo, esto es en el caso clásico \cite{LandauStat}. La mecánica cuántica impone restricciones aún mayores.
\\
Para poder superar este dilema se visualiza un enfoque estadístico, donde se ve que, a mayor número de partículas, empiezan a aparecer regularidades. Lo interesante de la mecánica estadística es que las leyes dadas solo funcionan cuando los grados de libertad son muchos.
\\
Sea un sistema mecánico macroscópico en un instante de tiempo $t$, se dice que el microestado del sistema está dado por cada $q_{i}(t)$ y $p_{i}(t)$. El microestado se puede representar por un punto $\lambda(t)$ en un espacio  abstracto de $6N$ dimensiones: el espacio de fase $\Gamma = \Pi^{3N}_{i} \{q_{i}, p_{i} \}$.
Con esto claro, la propuesta de Gibbs del ensamble es: Se suponen $N$ copias de un sistema macroscópico, cada uno de ellos viene representado en el espacio de fase $\Gamma$ por un punto $\lambda(t)$. Nótese que esto no significa  que las copias representan el mismo estado microscópico. Un sistema macroscópico puede tener diferentes microestados, y cada microestado puede cambiar con respecto a otro, por lo menos por una posición o momento. Esto no afectaría a las propiedades macroscópicas que se miden y, para términos prácticos, sería el mismo sistema macroscópico \cite{KardarStat}.
\\
El ensamble de copias permite crear una densidad de probabilidad en el espacio de fase. Sea $d\mathcal{N}(p,q,t)$ el número representativo de puntos en el volumen infinitesimal $d\Gamma = \Pi^{3N}_{i}dp_{i}dq_{i}$ alrededor del punto $(p,q)$. Entonces se define una densidad en el espacio de fase:
\begin{equation}
\rho(p,q,t)d\Gamma= \lim_{\mathcal{N} \to \infty} \frac{d\mathcal{N}(p,q,t)}{\mathcal{N}}.
\end{equation}
Es más conveniente escribir la evolución del sistema por esta densidad, por su sencillez matemática. 
Ahora que se pudo definir una densidad de probabilidad en el espacio de fase, se puede hallar los promedios de ensamble así:
\begin{equation}
\langle f \rangle = \int d\Gamma \rho(p,q,t) f(p,q).
\end{equation}
\\
Los promedios, se encuentran gracias al ensamble compuesto de varias copias idénticas del sistema, pero en microestados diferentes que son admisibles por los parámetros macroscópicos. Esto no significa que el sistema que se está estudiando se encuentre en todos esos estados al tiempo, es solo un método para lograr describir una densidad en el espacio de fase, que luego se usa para poder calcular propiedades del sistema.\\
Entonces cabe la pregunta de ¿qué tan posible es hablar de copias imaginarias del sistema, sabiendo que solo existe uno?
\\
\\
La mecánica estadística que introdujo Gibbs tenía como base la idea de la distribución microcanónica, esto es básicamente que: $\rho(q,p)$ es una distribución de densidad en el espacio de fase, la cual es cero en todo el espacio excepto en una superficie de energía, entre $E$ y $E+ \Delta E$, donde $\Delta E$ es bastante pequeña. En esta superficie de energía $\rho$ tiene un valor constante.
\\
La idea física detrás de esta distribución es simple de seguir. Desde la perspectiva de la teoría de probabilidad, cuando no se tiene conocimiento del problema que se quiere tratar, se supone que no hay razón para darle una mayor prioridad a un resultado que a otro, debido a la ignorancia subjetiva que se tiene. Entonces al no conocer nada del sistema se dice que todos los resultados tienen la misma probabilidad, luego, la distribución de probabilidad es constante.
\\
Entonces, si se tiene un sistema físico del que se conoce solo su estado macroscópico, este parámetro puede generar varios posibles estados microscópicos. Dado que no se tiene más información, se supone que cada posible estado microscópico tiene la misma probabilidad de ser el estado en el que el sistema se encuentra en realidad. Este es uno de los problemas de la fundamentación de la mecánica estadística, porque se basa en la ignorancia subjetiva que se tiene del sistema. Muchos argumentan que las teorías físicas deben ser edificadas en ideas objetivas, y el que una rama tan importante de la física se apoye en algo como la ignorancia propia deja mucho que pensar \cite{Penrose}.
\\
Aunque esta idea se base en la teoría de la probabilidad, también ha sido muy criticada en las matemáticas. Hay preguntas como: si se llegara a conocer un poco más del sistema, ¿aún sería válida la distribución microcanónica si la distribución de probabilidad realmente es diferente a una constante? Intentando dar respuesta a estas cuestiones sin tener, necesariamente, que construir toda una teoría, se propone cambiar el principio de probabilidades iguales por una idea más estable que tenga las mismas implicaciones que ese principio.
\\
En este punto se puede incorporar la entropía de Gibbs. Usando la notación, $d\Gamma \equiv d^{3}x_{1}...d^{3}p_{N}$, $d\Gamma_{1} \equiv d^{3}x_{1}d^{3}p_{1}$, $d\Gamma_{-1} \equiv d^{3}x_{2}...d^{3}p_{N}$; la función de distribución de puntos representativos es
\begin{equation}
\rho_{N}(x_{1},x_{2},..,x_{N};p_{1},p_{2},...,p_{N};t),
\end{equation}
la cual da la densidad de probabilidad del sistema en todo el espacio de fase. 
\\
La función H de Gibbs esta dada por
\begin{equation}
H_{G}= \int \rho_{N} \log \rho_{N} d \Gamma.
\end{equation}
Este punto es necesario para comparar la función H de Gibbs con la de Boltzmann, enfatizando que la función H de Boltzman está dada por
\begin{equation}
H_{B}=N \int f_{1} log f_{1} d\Gamma_{1},
\end{equation}
donde $f_{1}(x_{1},p_{1},t)$ es la probabilidad de densidad de una sola partícula, 
\begin{equation}
f_{1}(x_{1},p_{1},t)= \int \rho_{N} d \Gamma_{-1}.
\end{equation}
En el artículo escrito por Jaynes \cite{JaynesEntropies}, se demuestra que
\begin{equation}
H_{B} \leq H_{G},
\end{equation}
y la igualdad se cumple solo cuando 
\begin{equation}
\rho_{N}(x_{1},x_{2},...,x_{N};p_{1},p_{2},...,p_{N})=f_{1}(x_{1},p_{1})...f_{1}(x_{N},p_{N}),
\end{equation}
es decir, cuando hay independencia entre las partículas. Este es el caso para un gas que no tiene interacción entre partículas, gas ideal. Gracias a esto Jaynes prosigue mostrando que la entropía de Boltzmann, $S_{B}=k_{B}H_{B}$, solo es cierta cuando se habla de un fluido con la misma densidad y temperatura en todo el espacio pero sin fuerzas entre partículas. Mientras que la entropía de Gibbs, $S_{G}=k_{B}H_{G}$, es válida para cualquier sistema porque da la entropía fenomenológica de la termodinámica.
\\
Concluye que esta diferencia no puede ser pequeña porque hay interacciones entre partículas muy importantes que afectan ampliamente el resultado. Esto muestra que, aunque la idea de Gibbs sobre un ensamble es exótica, da los resultados esperados por la termodinámica, mientras que Boltzmann, usando ideas más intuitivas, no llega a hacer una conexión con la termodinámica.

\subsection{Ensamble Canónico}
Para explorar un poco más la idea de Gibbs, se presentará el ensamble canónico desde un punto de vista cuántico. Este ensamble será útil como referencia para los próximos capítulos. Para encontrar este operador se extremará la entropía de Gibbs bajo dos restricciones. La primera restricción es que el operador de densidad $\rho$ esté normalizado:
\begin{equation}
\Tr(\rho)=1.
\end{equation}
Además se impone la restricción de tener un valor de energía fijo,
\begin{equation}
\Tr(H \rho)= \langle E \rangle,
\end{equation}
donde H  es el operador hamiltoniano del sistema  \cite{ReichlStat}. El promedio de energía se escribe así, porque cuánticamente la traza permite escribir promedios de esta forma. La entropía de Gibbs para un sistema cuántico se escribe de la siguiente forma:
\begin{equation}
S=-k_{B} \Tr[\rho \ln (\rho)].
\end{equation}
Se ve que mantiene la forma de la entropía clásica, teniendo como diferencia que se cambia la integral por una traza. \\
Con ayuda de los multiplicadores de Lagrange, $\alpha$ y $\beta$, se encuentra la distribución de probabilidad que maximiza la entropía bajo las restricciones dadas. Entonces se tiene:
\begin{align}
\delta \{ \Tr [ \alpha \rho + \beta H \rho - k_{B} \rho \ln (\rho)] \} 
&= \Tr \{ [(\alpha - k_{B})I + \beta H - k_{B} \ln (\rho) ] \delta \rho \}=0.
\end{align}
Donde $I$ es el operador unitario. Como $\delta \rho$ es arbitrario, lo que está dentro de los corchetes cuadrados debe ser igual a 0. Organizando se llega al operador
\begin{equation}
\rho= \exp [ \bigg( \frac{ \alpha }{k_{B}}-1 \bigg) I + \frac{\beta}{k_{B}}H ].
\end{equation}
Por la condición de normalización que se tiene sobre el operador se encuentra
\begin{equation}
Z(\beta) \equiv \exp (1- \frac{ \alpha }{k_{B}}) = \Tr (e^{\frac{\beta H}{k_{B}}}).
\end{equation}
Aquí se ha definido la función de partición $Z(\beta)$ como la normalización del operador de densidad de probabilidad. Por medio de las restricciones se puede encontrar que el multiplicador $\beta$ es igual a $\frac{1}{T}$. Entonces se puede reescribir el operador de densidad  como
\begin{equation}
\rho = \frac{ e^{-\beta H}}{\Tr( e^{-\beta H})}.
\end{equation}

\section{Teorema de Liouville }
Construyendo sobre las ideas de Gibbs, se puede encontrar el teorema de Liouville. Este teorema tiene mucha importancia en la mecánica estadística, y para ver sus consecuencias en esta área primero se verá qué declara este teorema. La siguiente demostración del teorema sigue los pasos dados por Pathria en su libro \cite{PathriaStat}. 
\\
Se considera un volumen $\Gamma$ que encierra una región que se quiere estudiar en el espacio de fase, este volumen tiene una superficie $\sigma$. El cambio del número de puntos, microestados posibles del sistema, dentro de este volumen está dado por 
\begin{equation}
\frac{\partial}{\partial t} \int_{\Gamma} \rho d\Gamma,
\end{equation}

donde $\rho$ es la densidad en el espacio de fase definida anteriormente. En la ecuación anterior $d^{3N}q d^{3N}p=d\Gamma$. 
El cambio neto de puntos que salen de $\Gamma$ por la superficie $\sigma$ es

\begin{equation}
\int_{\sigma} \rho \mathbf{v \cdot \hat{n}} d\sigma,
\end{equation}

donde $\mathbf{v}$ es el vector de velocidad del punto representativo en la región de superficie $d\sigma$ y $\mathbf{\hat{n}}$ es el vector perpendicular a esta superficie con dirección de salida. Por el teorema de la divergencia se tiene:

\begin{equation}
\int_{\Gamma} \div{ ( \rho\mathbf{v} ) } d\Gamma = \int_{\Gamma} \sum_{i=1}^{3N} \Big( \frac{\partial}{\partial q_{i}}(\rho \dot{q_{i}})+ \frac{\partial}{\partial p_{i}} (\rho \dot{p_{i}}) \Big) d\Gamma
\end{equation}
Debido a que la cantidad de puntos se conserva en el espacio de fase, el ensamble que se considera no agrega nuevos miembros ni elimina los que ya se encuentran en éste, lo que permite concluir:
\begin{equation}
 \int_{\Gamma} \Big( \frac{\partial \rho}{\partial t} + \div{ ( \rho\mathbf{v} ) } \Big) d\Gamma =0.
\end{equation}
La condición para que esta integral sea cierta para cualquier $\Gamma$ es que el integrando sea cero. Esto nos da la ecuación de continuidad para el espacio de fase,

\begin{equation} \label{con}
 \frac{\partial \rho}{\partial t} + \div{ ( \rho\mathbf{v} ) }=0
\end{equation}

usando la forma explícita de la divergencia,
\begin{equation}
 \frac{\partial \rho}{\partial t} +\sum_{i=1}^{3N} \Big( \frac{\partial\rho}{\partial q_{i}}\dot{q_{i}}+  \frac{\partial\rho}{\partial p_{i}}\dot{p_{i}} \Big) + \rho \sum_{i=1}^{3N} \Big( \frac{\partial \dot{q_{i}}}{\partial q_{i}} + \frac{\partial \dot{p_{i}}}{\partial p_{i}}\Big)=0.
\end{equation}
Por las ecuaciones de Hamilton \ref{Hamilton} el último término se cancela. Como $\rho$ depende de $p,q$ y $t$, se puede organizar con los dos primeros términos para que la ecuación quede de la siguiente forma:
\begin{equation} \label{liouville}
\frac{d \rho}{dt}= \frac{\partial \rho}{\partial t} + [ \rho, H ]=0.
\end{equation}
La ecuación \ref{liouville} es el teorema de Liouville. Este dice que la densidad "local" de puntos vista desde un observador que se mueve con el punto se mantiene constante en el tiempo. Luego, los puntos en el espacio de fase se mueven de la misma manera que un fluido incompresible en el espacio físico.

Esta conclusión es la más clara al obtener el teorema de Liouville pero también hay consecuencias profundas dadas por éste. Para ver las consecuencias que brinda el teorema seguiremos a Kardar \cite{KardarStat}.

\subsection{Consecuencias del Teorema de Liouville}
La primera consecuencia es que al hacer una inversión temporal el corchete de Poisson $[\rho, H]$ cambia de signo y esto predice que la densidad revierte su evolución. Es decir que al hacer la transformación $(p,q,t) \to (p,q,-t)$ el teorema de Liouvile implica $\rho(p,q,t)=\rho(-p,q,-t)$.
La segunda es sobre la evolución del promedio de ensamble en el tiempo.
\begin{equation} \label{corchete}
\frac{d \langle f \rangle}{dt}= \int d \Gamma \frac{\partial \rho(p,q,t)}{\partial t} f(p,q)= \sum_{i=1}^{3N} \int d\Gamma f(p,q) \Big( \frac{\partial \rho}{\partial p_{i}}\frac{\partial H}{\partial q_{i}} - \frac{\partial \rho}{\partial q_{i}}\frac{\partial H}{\partial p_{i}}  \Big)
\end{equation}
En la ecuación anterior se usó el teorema de Liouville poniendo explícitamente el corchete de Poisson. Integrando por partes y recordando que $\rho$ tiende a cero en los límites de la integración, se llega a:
\begin{equation}
\frac{d \langle f \rangle}{dt}= \langle [f,H] \rangle.
\end{equation}
Con esta relación sobre los promedios se puede ver qué condiciones son necesarias para que el ensamble se encuentre en el estado de equilibrio. Dados unos parámetros macroscópicos se sabe que si el ensamble corresponde a uno en el equilibrio, los promedios de ensamble deben ser independientes del tiempo. Esto se sigue de \ref{corchete} encontrando 
 
\begin{equation}
[\rho_{eq}, H]=0.
\end{equation}
Una posible solución a la ecuación anterior es que $\rho_{eq}(p,q)=\rho(H(p,q))$. Es una solución ya que $[\rho(H),H]=\rho^{'}(H)[H,H]=0$. Esto muestra el supuesto básico de la mecánica estadística, $\rho$ es constante en las superficie de energías constantes $H$. El supuesto de la mecánica estadística dictamina que el macroestado está dado por una densidad uniforme de microestados. Esto es el principio de probabilidades iguales a priori.
\\
La consecuencia anterior respondería la pregunta de cómo definir equilibrio para partículas en movimiento. Pero para saber si todos los sistemas evolucionan naturalmente al equilibrio y justificar el supuesto básico de la mecánica estadística, se debe mostrar que las densidades no estacionarias se van acercando a las densidades estacionarias $\rho_{eq}$. Pero esto entra en conflicto con la primera consecuencia (inversión temporal), dada una densidad $\rho(t)$ que se acerque a $\rho_{eq}$ habrá otra, dada la inversión temporal que se estará alejando de esta densidad de equilibrio. Lo que se ha propuesto normalmente es mostrar que $\rho(t)$ se encontrará en el intervalo cercano a $\rho_{eq}$ una gran parte del tiempo, lo cual significa que los promedios temporales están dominados por la solución estacionaria. Esto nos lleva al problema de ergodicidad, ¿es válido igualar el promedio temporal con el promedio de ensamble?
\\

\section{Ergodicidad}
En esta sección se hablará un poco sobre el problema de ergodicidad. Siguiendo a \cite{TodaStat}, se plantearán las dificultades y se verán algunas soluciones que se han dado para este problema. Se habla de este problema ya que la hipótesis de ergodicidad es fundamental para la mecánica estadística. Además, la nueva perspectiva que se mostrará en los siguientes capítulos \cite{Popescu2006}, no hace énfasis en esta hipótesis; es más, ni siquiera llega a ser una pregunta que necesite ser resuelta. Para ver por qué facilita tanto evitar esta hipótesis se dará esta sección.
\\
La mecánica estadística tiene como base el principio de probabilidades iguales. En la literatura generalmente se empieza hablando de cómo se acepta el principio desde la teoría de la probabilidad. Pero hay un concepto que casi nunca es profundizado por los textos usados regularmente; el porqué se debe introducir la probabilidad en estos casos.\\
Autores como Kerson Huang y Lev Landau empiezan explicando cómo se manejaría un problema de muchas partículas desde la forma clásica. La idea que muestran es la forma habitual de introducir la mecánica estadística: se presenta que describir avogadro número de partículas es complicado aunque se tengan las ecuaciones y se pueda de alguna forma (numérica o computacional) resolver todas las ecuaciones que salen. Entonces se sigue explicando que para resolver el problema se tiene en cuenta que la complejidad de las interacciones que hay entre el sistema y el ambiente, hace que el sistema se encuentre en muchos estados varias veces. Esto otorga un comportamiento probabilístico al sistema que puede llegar a comportarse como uno aislado.
\\ 
Estos argumentos introducen las probabilidades al sistema mecánico sin tener que entrar en un conflicto con la mecánica clásica. Nótese que luego de dada esa explicación, siguen a formalizar lo dicho y construyen la mecánica estadística que se conoce. Pero hay otra forma de introducir las probabilidades a estos sistemas de muchos grados de libertad y es por hipótesis; el libro escrito por Aleksandr Khinchin \cite{KhinchinStat} sigue esta filosofía. Aunque haya dos formas de establecer las probabilidades, de igual forma se llega al problema de ergodicidad. 
\\
La hipótesis de ergodicidad afirma que el promedio temporal de una cantidad mecánica en un sistema aislado es igual al promedio de ensamble. Dada una teoría que explique algún fenómeno que nos interese, se querrá probar qué tan buenos resultados se obtienen de ella. Para esto se comparará con el experimento, pero las cantidades físicas en una teoría aparecen como funciones de las variables dinámicas. Ya se ve un problema claro en este procedimiento: ¿cómo es posible comparar los datos medidos en el laboratorio con las funciones de la teoría? Las funciones de las variables dinámicas toman valores diferentes para varios estados del sistema, pero para poder comparar los resultados experimentales se debería saber cuál es el estado del sistema, es decir, es necesario dar todas las variables dinámicas para poder estar seguros de que se están comparando los datos con el estado específico. Sin embargo, como ya se ha dicho, esto es imposible.
\\
Hay un detalle en este caso y es que al medir una cantidad física esta no se observa de manera instantánea, se necesita algún tiempo $\tau$ para ser observada. En el caso de una cantidad termodinámica, el tiempo en que se demora midiendo va desde $\tau_{0}$, que es el tiempo necesario para que no hayan correlaciones del movimiento molecular, y  $\tau_{m}$ que es el tiempo máximo en el que la propiedad macroscópica que se mide no cambia. Al medir la temperatura, es necesario esperar un tiempo $\tau_{0}$ para que se estabilicen el sistema y el termómetro, y la medición se puede tomar hasta $\tau_{m}$ cuando el sistema se vuelve a perturbar. Se supone que cualquier cantidad que se mida durante este intervalo no debe depender de $\tau$. En algunos sistemas que llegan al equilibrio $\tau_{m}$ puede extenderse al infinito.
\\
Entonces, con lo dicho anteriormente, se puede proponer la siguiente idea de medición: sea $P$ un punto en el espacio de fase, y sea $f(P,t)$ una función sobre el espacio de fase; si esta función corresponde a una cantidad termodinámica no debe depender de su estado inicial, porque sistemas termodinámicos con cantidades termodinámicas iguales se pueden encontrar en estados dinámicos diferentes. \\
La relación de esta función con los parámetros está dada por un promedio, en este caso, el promedio temporal, luego:
\begin{equation}
\overline{f_{\tau}}(P)= \frac{1}{\tau} \int_{0}^{\tau} f(P,t) dt,
\end{equation}
 
si se está en el equilibrio

\begin{equation}
\overline{f}(P)= \lim_{\tau \to \infty} \frac{1}{\tau} \int_{0}^{\tau} f(P,t) dt.
\end{equation}
\\
Para poder mantener consistencia con la termodinámica los promedios $\overline{f_{\tau}}(P)$ y  $\overline{f}(P)$ deben ser independientes de $\tau$. Además en el equilibrio los dos promedios deben ser iguales, esto es muy grande para $\tau_{m}$. \\
El promedio de ensamble está definido así:

\begin{equation}
\langle f \rangle = \frac{1}{\Omega} \int_{\Gamma} f(P,t) \rho d\Gamma \quad ; \quad \Omega= \int_{\Gamma} d\Gamma.
\end{equation}
La integral es sobre el espacio accesible en el espacio de fase, $d\Gamma$ es el elemento de volumen. Como el punto $P$ evoluciona gracias a las ecuaciones de Hamilton se preserva la medida por el teorema de Liouville, luego $\langle f \rangle$ no depende del tiempo. Entonces 
\begin{equation}
\langle f \rangle =  \overline{\langle f \rangle}.
\end{equation}
Como el promedio temporal conmuta con el promedio de ensamble, hay casos especiales en los que no ocurre esto. Entonces
\begin{equation}
\overline{\langle f \rangle} = \langle \overline{f} \rangle.
\end{equation}
Como $\overline{f}$ es independiente de $P$ y constante sobre todo el espacio de fase, entonces $\langle \overline{f} \rangle= \overline{f}$. Por lo tanto
\begin{equation} \label{ergodicidad}
\langle f \rangle = \overline{f}.
\end{equation}
Aquí se ha mostrado por qué es válido el uso del promedio de ensamble en vez del temporal. Para esta demostración se han usado dos suposiciones. Primero, que $\overline{f}(P)$ tiene el mismo valor para cualquier $P$ en la misma trayectoria que este punto traza, lo que \cite{Ehrenfest} llama el G-Path. Birkhoff demostró esta aseveración \cite{Birkhoff}. El problema es saber si $\overline{f}(P)$  tiene un valor definido independiente de la condición inicial $P$, y si esto se satisface para $P$ en cualquier G-path. \\
Según M. Toda \textit{et al} en su libro \cite{TodaStat}, Boltzmann propuso que para satisfacer la igualdad \ref{ergodicidad}, el G-path debe pasar por todos los puntos de la superficie de energía. Como la trayectoria del punto pasa por todos los puntos transcurrido suficiente tiempo, el promedio temporal es igual al promedio de ensamble. Esto es lo que Boltzmann llamó la hipótesis de ergodicidad. Esta idea sobre la trayectoria del punto $P$ es llamada la condición de ergodicidad en el sentido de Boltzmann.
\\ 
Como la trayectoria dinámica es un conjunto unidimensinal de puntos continuos y nunca se intersecta a sí misma en una superficie multidimensional constante de energía, se hace imposible una correspondencia uno a uno de un espacio de dimensión dos o mayor a un espacio unidimensional. Luego, la ergodicidad en el sentido de Boltzmann no puede concebirse. La igualdad \ref{ergodicidad} no siempre implica ergodicidad en el sentido de Boltzmann.
\\
\\
Aunque la hipótesis ergódica se encuentre en un punto importante en la construcción de la física estadística, hay sistemas que no son ergódicos pero aún así se pueden tratar con la mecánica estadística. El calor específico de los sólidos pueden ser modelado de manera correcta bajo el supuesto de una red de partículas cuánticas vibrando en una aproximación armónica.  El tratamiento clásico también es satisfactorio para temperaturas altas. Como se sabe, cualquier modo normal en vibraciones armónicas es independiente de los otros modos normales y la energía de cada modo es constante. Se puede describir el calor específico por medio de la aproximación armónica, pero si existen términos no lineales pequeños o anarmónicos, habrá un intercambio de energía entre los modos normales. 
\\
Existe un problema famoso que E. Fermi, J. Pasta y S. Ulam investigaron \cite{FermiPastaUlam}, un sistema de una cadena lineal de $N+1$ partículas idénticas de masa $m$ conectadas por resortes idénticos. Pero en este caso ellos incluyeron interacciones no lineales al hamiltoniano de este sistema, que puede ser escrito 
\begin{equation}
H= \sum_{k} \frac{1}{2m} p_{k}^{2} +\frac{\kappa}{2}\sum_{k} (q_{k+1}-q_{k})^{2}+\frac{\lambda}{s}\sum_{s} (q_{k+1}-q_{k})^{s},
\end{equation}
donde $\lambda$ es la constante de acoplamiento y $s=3,4$; esto representa los potenciales no lineales cúbicos y cuadráticos. Fermi, Pasta y Ulam querían demostrar la intuición que tenían de que cualquier interacción no lineal entre partículas de un sistema generaría un comportamiento ergódico o irreversible para el sistema. Las ecuaciones de movimiento del sistema son
\begin{equation}
m \ddot{q}_{k}= \kappa (q_{k+1}-2q_{k}+q_{k-1})+ \lambda[(q_{k+1}-q_{k})^{s-1}-(q_{k}-q_{k-1})^{s-1}],
\end{equation} 
o transformando a coordenadas normales: $q_{k}=(\frac{2}{N})^{\frac{1}{2}} \sum_{j=1}^{N-1}x_{j} \sin (\frac{jk \pi }{N})$, $m\dot{x}_{j}=y_{j}$ la ecuación queda como
\begin{equation}
\ddot{x}_{j}= - \omega_{j}^{2} x_{j} +\lambda F_{j}(x),
\end{equation}
con $\omega_{j} =2(\frac{\kappa }{m})^{\frac{1}{2}} \sin (\frac{j \pi}{2N})$. Donde $F_{j}(x)$ son las fuerzas no lineales. El sistema que ellos estudiaron fue de $N=32$ y $N=64$, mirando su evolución luego de un largo tiempo. Las fuerzas que usaron fueron cuadráticas, cúbicas y lineales. Además, la energía de los modos está dada por 
\begin{equation}
E_{k} = \frac{1}{2} m(\dot{x}^{2}_{k}+ \omega_{k}^{2}x_{k}^{2}).
\end{equation}
Simularon cómo las energías cambiaban en el tiempo cuando inicialmente se le da al menor modo una cantidad de energía. Se esperaba que la energía dada al menor modo fuera siendo distribuida paulatinamente a los demás modos, y al final se tuviese una cantidad de energía igualmente distribuida en cada modo. Pero este no fue el hallazgo para $N=32$ con un potencial cúbico y con las constantes $\kappa=m=1$ y $\lambda=1/4$. Las condiciones iniciales eran: las velocidades iguales a 0 y los desplazamientos dados por la curva sinusoidal del menor modo normal. Lo que se vio fue que la energía regresaba al primer modo de manera periódica; la energía era intercambiada en los modos más bajos, pero no mucho en los modos más grandes. Se llegó a la conclusión de que el sistema regresa de forma casi exacta a su estado inicial.  \\
Entonces la presencia de anarmonicidad no implica ergodicidad para este sistema. Esto es un ejemplo de un sistema que no es ergódico, lo que dejó mucha desconformidad dado que Fermi, Pasta y Ulam  no lograron  ver por qué la hipótesis que se usa para formar la mecánica estadística no se cumple para sistemas que comúnmente  son usados en la física. Este cuestionamiento es uno de los aspectos que contribuye que el marco teórico de la física estadística sea inestable en comparación con otras ramas de la física.


\section{Jaynes}
Los problemas planteados hasta ahora han sido diversos y de gran complejidad. Por esto unos cuantos intentos se han hecho para proponer una nueva perspectiva que no tenga estas tensiones. Han existido muchas propuestas nuevas, buscando basar la mecánica estadística en otras ideas. En esta sección se mostrará la visión de E.T Jaynes. Jaynes propone una relación entre la mecánica estadística y la teoría clásica de la información \cite{JaynesI},\cite{JaynesII}, lo que perfila una luz al usar herramientas de la información en áreas de la física.
\\
Jaynes, impulsado por la ausencia de un argumento sólido que conectara los fenómenos microscópicos y los macroscópicos, planteó que para tener una mecánica estadística libre de objeciones, ésta debería seguir los siguientes requerimientos: estar libre de incongruencias matemáticas, no tener suposiciones externas arbitrarias y ser capaz de describir procesos tanto fuera del equilibrio como en equilibrio.
\\
Jaynes explica que la segunda condición parece extraña al hablar de una teoría física, porque en general se proponen hipótesis que luego intentan ser explicadas. Pone el ejemplo de la unidad de volumen en el espacio de fase, que inicialmente fue puesta para poder conseguir los valores correctos de la presión de vapor en equilibrio. Luego, con la ayuda de la teoría cuántica se visibilizó que esta unidad salía naturalmente de las leyes físicas, lo que hace que Jaynes proponga que la mecánica estadística sea un ejemplo de inferencia estadística. Como la expresión $- \sum p_{i} \log p_{i}$ aparece en la teoría de la información -que es una teoría de inferencia estadística- y en la mecánica estadística, Jaynes expone cómo crear una conexión entre las dos teorías haciendo que esta fórmula tenga el mismo concepto y así poder aplicar la teoría de la información clásica a la mecánica estadística. Con esto, Jaynes resalta  una simplificación de la matemática, afirmando que la mecánica estadística puede afrontar problemas más generales y nuevos problemas físicos.
\\
Toda la perspectiva de Jaynes se basa en el principio de máxima entropía. Para poder entender este principio se plantea un problema que ocurre en muchos casos, no solo en la física. Sea $x$ una cantidad discreta que puede tener valores $x_{i}$ con $i=1, 2,,n$. No se saben las probabilidades correspondientes $p_{i}$. Lo único que se conoce del problema son los promedios de la función $f(x)$,
\begin{equation}
\langle f(x) \rangle =\sum_{i}^{N} p_{i} f(x_{i}).
\end{equation}
Entonces la pregunta que se hace es: con esta información, ¿cuál es el valor esperado de la función $g(x)$? Problemas de este tipo aparecen en muchas áreas -como la teoría de la información-, porque son típicos de la teoría de la probabilidad. Encontrar las probabilidades con poca información es un problema en el que trabajaron los padres de la probabilidad. El principio dado por Laplace es un intento de resolver el problema. Este principio asigna probabilidades iguales a los eventos. Jaynes expresa que, a menos que haya una simetría en el problema, se encuentran paradojas en los casos continuos; por eso, se ha dejado de usar este tipo de formulaciones en los problemas actuales, continúa diciendo Jaynes.
\\
Para continuar exponiendo el planteamiento de Jaynes, se debe entender que la perspectiva que acepta sobre la teoría de la probabilidad es subjetiva. Esto quiere decir que, como J.M. Keynes o H. Jeffreys aceptan, las probabilidades son una cuantificación de la ignorancia humana. La probabilidad es solo una medida basada en la información que se tiene sobre el posible resultado que puede salir. Entonces la distribución de probabilidad solo es la representación del estado actual de nuestro conocimiento. \\
En contraposición con la perspectiva objetiva que dice que la probabilidad es la proporción de frecuencias de un experimento, calculando la distribución de probabilidad, se está haciendo una predicción que se puede en principio comparar con el experimento. Esta perspectiva acepta las probabilidades como algo externo al ser humano. Jaynes dice que la visión subjetiva es más general, porque las proporciones de frecuencia siempre pueden interpretarse desde esta perspectiva. Además, manifiesta que la posición subjetiva acepta interrogantes que para el objetivismo no tienen importancia.
\\
Shannon, en su teoría de la información, propuso una función que describía de forma única y sin ambigüedad la cantidad de incertidumbre que tiene una distribución de probabilidad \cite{ShannonInformation}. Esta cantidad es positiva, se incrementa con una incertidumbre decreciente, y es aditiva para fuentes de incertidumbres independientes. Esta es
\begin{equation}
H(p_{1},...,p_{N})=-k \sum_{i} p_{i} \ln p_{i},
\end{equation}
donde $k$ es una constante positiva. Aquí, Jaynes hace la conexión y es un punto importante. A esta cantidad, Shannon la llama entropía, pero no es la misma entropía termodinámica. Shannon dice en Scientific American (volumen 225 del año 1970  página 180) que Von Neumann le propuso este nombre, ya que nadie sabía realmente qué era la entropía. Pero Jaynes hace el puente entre las dos teorías, diciendo que la entropía termodinámica es la cantidad de incertidumbre que se tiene del sistema. Esto es lo que quiere decir el principio de la máxima entropía: dadas unas restricciones, o sea, con una información parcial, se debe encontrar la distribución que minimice la incertidumbre. Esto es equivalente a maximizar $H$ bajo las condiciones conocidas. Esta es la forma de llegar a la distribución sin usar suposiciones arbitrarias.
\\
Jaynes prosigue usando multiplicadores de Lagrange y obtiene los siguientes resultados para un número $m$ de funciones, dado  $\langle f_{r}(x) \rangle = \sum_{i} p_{i} f_{r} (x_{i})$:
\begin{equation} \label{Boltzmann maximum}
p_{i}= \exp \{ -[\lambda_{0}+ \lambda_{1}f_{1}(x_{i})+...+\lambda_{m} f_{m}(x_{i})] \},
\end{equation}
donde los $\lambda$'s son los multiplicadores de Lagrange. Además se define la función de partición como
\begin{equation}
Z(\lambda_{1},...,\lambda_{m})=\sum_{i} \exp \{ -[ \lambda_{1}f_{1}(x_{i})+...+\lambda_{m} f_{m}(x_{i})] \}.
\end{equation}
Los multiplicadores de Lagrange se pueden encontrar por
\begin{align*}
\langle f_{r}(x) \rangle  &= - \frac{\partial}{\partial \lambda_{r}} \ln Z, \\
\quad \lambda_{0} &= \ln Z.
\end{align*}
Jaynes argumenta que, siguiendo este principio, se llega a una distribución de probabilidad imparcial correspondiente a la información del problema, de forma inequívoca y única. Dicho principio también permite la modificación de la distribución de probabilidad si se llega a conocer más información.
\\
Gracias a esto se pueden ver los resultados dados por la mecánica estadística fácilmente. Sea $E(\alpha_{1},\alpha_{2},...)$ los niveles de energía de un sistema, donde los $\alpha$'s son parámetros externos como el volumen o potenciales gravitacionales, si solo se conoce $\langle E \rangle $. Pero por la ecuación \ref{Boltzmann maximum} ya se saben las probabilidades de los niveles de energías $E_{i}$; ésta es la distribución de Maxwell-Boltzmann. Entonces se pueden seguir identificando los distintos parámetros de la termodinámica:
\begin{align*}
\lambda_{1} &=\frac{1}{kT}, \\
U-TS &= F(T,\alpha_{1},\alpha_{2},...)=-kT \ln Z(T,\alpha_{1},\alpha_{2},...), \\
S &= -\frac{\partial F}{\partial T}=-k\sum_{i} p_{i} \ln p_{i}, \\
\beta_{i} &= kT \frac{\partial }{\partial \alpha_{i}} \ln Z.
\end{align*}
$\beta_{i}$ se puede ver como una fuerza generalizada. Para casos específicos llega a ser la presión, el tensor de estrés, los momentos magnéticos y/o  eléctricos. Se puede también encontrar la distribución gran canónica solo poniendo una nueva restricción sobre el número de partículas, sea $n_{i}$ el número de partículas del tipo $i$. Además, para especificar el estado del sistema, se necesita las $n$'s junto con un nivel de energía $E(\alpha_{1},\alpha_{2},...;n_{1}n_{2}...)$ y se conocen los valores esperados de la energía y cada número de partículas de todos los tipos.
\begin{equation}
\langle E \rangle \quad , \quad \langle n_{i} \rangle \quad , \quad i=  \{ 1,2,... \}.
\end{equation}
la función de partición es la de la gran canónica
\begin{equation}
Z(\alpha_{1},\alpha_{2},...|\lambda_{1} \lambda_{2} ,...,\beta )= \sum_{n_{1} n_{2}...} \sum_{i} \exp \{ -[ \lambda_{1} n_{1}+\lambda_{2} n_{2}+...+\beta E_{i} (\alpha_{k} | n_{s} ) ] \}.
\end{equation}
La propuesta de Jaynes ha ganado popularidad, y evita preguntarse cuál es la distribución sobre la que se están muestreando las probabilidades, porque como su perspectiva es subjetiva, lo único que se tiene en cuenta es la ignorancia subjetiva. No se debe crear un ensamble de copias para darle un significado a la probabilidad.



